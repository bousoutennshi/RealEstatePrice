"""
ãƒ–ãƒ©ãƒ³ã‚ºã‚¿ãƒ¯ãƒ¼è±Šæ´² 2LDKç‰©ä»¶ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®ä¸å‹•ç”£ã‚µã‚¤ãƒˆã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’åé›†ã—ã€JSONå½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚
"""
import json
import sys
from pathlib import Path
from datetime import datetime

from utils import setup_logger, DataManager
from scrapers import SuumoScraper, HomesScraper, AthomeScraper, RehouseScraper, LivableScraper


def load_config(config_path: str = 'config/config.json') -> dict:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    
    Returns:
        dict: è¨­å®šæƒ…å ±
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    logger = setup_logger('main', 'logs/scraping.log')
    logger.info("=" * 60)
    logger.info("ãƒ–ãƒ©ãƒ³ã‚ºã‚¿ãƒ¯ãƒ¼è±Šæ´² 2LDKç‰©ä»¶ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 60)
    
    # è¨­å®šã®èª­ã¿è¾¼ã¿
    try:
        config = load_config()
        logger.info(f"Target property: {config['property']['name']}")
        logger.info(f"Layout: {config['property']['layout']}")
    except FileNotFoundError:
        logger.error("Config file not found: config/config.json")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse config file: {e}")
        sys.exit(1)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    data_manager = DataManager(config)
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–
    scrapers = [
        SuumoScraper(config),
        HomesScraper(config),
        AthomeScraper(config),
        RehouseScraper(config),
        LivableScraper(config),
    ]
    
    # å„ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    all_data = []
    for scraper in scrapers:
        try:
            logger.info(f"\n--- {scraper.get_source_name()} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ ---")
            listings = scraper.scrape()
            
            if listings:
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                raw_file = data_manager.save_raw_data(
                    scraper.get_source_name().lower().replace(' ', '_'),
                    listings
                )
                logger.info(f"Raw data saved: {raw_file}")
                
                all_data.append({
                    'source': scraper.get_source_name(),
                    'listings': listings
                })
            else:
                logger.warning(f"No data collected from {scraper.get_source_name()}")
        
        except Exception as e:
            logger.error(f"Error scraping {scraper.get_source_name()}: {e}", exc_info=True)
            continue
    
    # ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
    if all_data:
        logger.info("\n--- ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆå‡¦ç†ã‚’é–‹å§‹ ---")
        merged_listings = data_manager.merge_data(all_data)
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        processed_file = data_manager.save_processed_data(
            merged_listings,
            config['property']['name']
        )
        
        logger.info("=" * 60)
        logger.info("ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"ç·ç‰©ä»¶æ•°: {len(merged_listings)}ä»¶")
        logger.info(f"ä¿å­˜å…ˆ: {processed_file}")
        logger.info("=" * 60)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ç‰©ä»¶å: {config['property']['name']}")
        print(f"é–“å–ã‚Š: {config['property']['layout']}")
        print(f"åé›†ã‚µã‚¤ãƒˆæ•°: {len(all_data)}ã‚µã‚¤ãƒˆ")
        print(f"ç·ç‰©ä»¶æ•°: {len(merged_listings)}ä»¶")
        print(f"\nå„ã‚µã‚¤ãƒˆã‹ã‚‰ã®åé›†æ•°:")
        for data in all_data:
            print(f"  - {data['source']}: {len(data['listings'])}ä»¶")
        print(f"\nä¿å­˜å…ˆ: {processed_file}")
        print("=" * 60)
        
    else:
        logger.warning("No data collected from any source")
        print("\nâš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == '__main__':
    main()

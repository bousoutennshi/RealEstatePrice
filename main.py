"""
ãƒ–ãƒ©ãƒ³ã‚ºã‚¿ãƒ¯ãƒ¼è±Šæ´² 2LDKç‰©ä»¶ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®ä¸å‹•ç”£ã‚µã‚¤ãƒˆã‹ã‚‰ç‰©ä»¶æƒ…å ±ã‚’åé›†ã—ã€JSONå½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚
"""
import json
import sys
from pathlib import Path
from datetime import datetime

from utils import setup_logger, DataManager
from scrapers import SuumoScraper, HomesScraper, AthomeScraper, RehouseScraper, LivableScraper


def load_config(config_path: str = 'config/config.json') -> dict:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    
    Returns:
        dict: è¨­å®šæƒ…å ±
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    logger = setup_logger('main', 'logs/scraping.log')
    logger.info("=" * 60)
    logger.info("ãƒ–ãƒ©ãƒ³ã‚ºã‚¿ãƒ¯ãƒ¼è±Šæ´² ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 60)
    
    # è¨­å®šã®èª­ã¿è¾¼ã¿
    try:
        config = load_config()
        logger.info(f"Target property: {config['property']['name']}")
        logger.info(f"Layouts: {', '.join(config['property']['layouts'])}")
    except FileNotFoundError:
        logger.error("Config file not found: config/config.json")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse config file: {e}")
        sys.exit(1)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    data_manager = DataManager(config)
    
    # å…¨LDKã‚¿ã‚¤ãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    all_layouts_data = []
    
    for layout in config['property']['layouts']:
        logger.info(f"\n{'=' * 60}")
        logger.info(f"é–“å–ã‚Š: {layout} ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹")
        logger.info(f"{'=' * 60}")
        
        # ç¾åœ¨ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”¨ã«è¨­å®šã‚’ä¸€æ™‚çš„ã«æ›´æ–°
        current_config = config.copy()
        current_config['property'] = config['property'].copy()
        current_config['property']['layout'] = layout
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–
        scrapers = [
            SuumoScraper(current_config),
            HomesScraper(current_config),
            AthomeScraper(current_config),
            RehouseScraper(current_config),
            LivableScraper(current_config),
        ]
        
        # å„ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        layout_data = []
        for scraper in scrapers:
            try:
                logger.info(f"\n--- {scraper.get_source_name()} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ ---")
                listings = scraper.scrape()
                
                if listings:
                    # å„ç‰©ä»¶ã«layoutæƒ…å ±ã‚’è¿½åŠ ï¼ˆã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ï¼‰
                    for listing in listings:
                        if 'layout' not in listing:
                            listing['layout'] = layout
                    
                    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    raw_file = data_manager.save_raw_data(
                        scraper.get_source_name().lower().replace(' ', '_'),
                        listings,
                        layout
                    )
                    logger.info(f"Raw data saved: {raw_file}")
                    
                    layout_data.append({
                        'source': scraper.get_source_name(),
                        'listings': listings
                    })
                else:
                    logger.warning(f"No data collected from {scraper.get_source_name()}")
            
            except Exception as e:
                logger.error(f"Error scraping {scraper.get_source_name()}: {e}", exc_info=True)
                continue
        
        # ã“ã®LDKã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ä½“ãƒªã‚¹ãƒˆã«è¿½åŠ 
        all_layouts_data.extend(layout_data)
        
        logger.info(f"\n{layout} ã®ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {sum(len(d['listings']) for d in layout_data)}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
    if all_layouts_data:
        logger.info("\n" + "=" * 60)
        logger.info("å…¨LDKã®ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã‚’é–‹å§‹")
        logger.info("=" * 60)
        merged_listings = data_manager.merge_data(all_layouts_data)
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        processed_file = data_manager.save_processed_data(
            merged_listings,
            config['property']['name']
        )
        
        logger.info("=" * 60)
        logger.info("ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"ç·ç‰©ä»¶æ•°: {len(merged_listings)}ä»¶")
        logger.info(f"ä¿å­˜å…ˆ: {processed_file}")
        logger.info("=" * 60)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ç‰©ä»¶å: {config['property']['name']}")
        print(f"é–“å–ã‚Š: {', '.join(config['property']['layouts'])}")
        print(f"åé›†ã‚µã‚¤ãƒˆæ•°: {len(set(d['source'] for d in all_layouts_data))}ã‚µã‚¤ãƒˆ")
        print(f"ç·ç‰©ä»¶æ•°: {len(merged_listings)}ä»¶")
        
        # LDKåˆ¥ã®ä»¶æ•°ã‚’è¡¨ç¤º
        print(f"\né–“å–ã‚Šåˆ¥ã®åé›†æ•°:")
        layout_counts = {}
        for listing in merged_listings:
            layout = listing.get('layout', 'Unknown')
            layout_counts[layout] = layout_counts.get(layout, 0) + 1
        for layout in sorted(layout_counts.keys()):
            print(f"  - {layout}: {layout_counts[layout]}ä»¶")
        
        print(f"\nã‚µã‚¤ãƒˆåˆ¥ã®åé›†æ•°:")
        source_counts = {}
        for data in all_layouts_data:
            source = data['source']
            source_counts[source] = source_counts.get(source, 0) + len(data['listings'])
        for source in sorted(source_counts.keys()):
            print(f"  - {source}: {source_counts[source]}ä»¶")
            
        print(f"\nä¿å­˜å…ˆ: {processed_file}")
        print("=" * 60)
        
    else:
        logger.warning("No data collected from any source")
        print("\nâš ï¸  ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")



if __name__ == '__main__':
    main()
